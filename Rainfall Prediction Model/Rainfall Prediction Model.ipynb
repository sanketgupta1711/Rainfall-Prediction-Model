{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_data = pd.read_csv('weatherAUS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = import_data\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.RainTomorrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df), '----> Number of Samples in the Dataset')\n",
    "print(df['RainTomorrow'].isnull().sum(), '----> Number of Empty Target Values (RainTomorrow)')\n",
    "print(df['RainToday'].isnull().sum(), '---> Number of Empty RainToday Values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing with 0 and 1 for convenience\n",
    "df['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\n",
    "df['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing Target Values\n",
    "df['RainToday'] = df['RainToday'].fillna(df['RainToday'].mode()[0])\n",
    "df['RainTomorrow'] = df['RainTomorrow'].fillna(df['RainTomorrow'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['RainTomorrow'].isnull().sum())\n",
    "print(df['RainToday'].isnull().sum())\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.RainTomorrow.value_counts(normalize = True).plot(kind='bar')\n",
    "plt.title('Plot to view imbalance in this dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = df['Date'].astype('category').cat.codes\n",
    "df['Location'] = df['Location'].astype('category').cat.codes\n",
    "df['WindGustDir'] = df['WindGustDir'].astype('category').cat.codes\n",
    "df['WindDir9am'] = df['WindDir9am'].astype('category').cat.codes\n",
    "df['WindDir3pm'] = df['WindDir3pm'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "ImputedDF = df.copy(deep=True) \n",
    "mice_imputer = IterativeImputer()\n",
    "ImputedDF.iloc[:, :] = mice_imputer.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ImputedDF.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Outliers\n",
    "from scipy import stats\n",
    "CorrDF = ImputedDF[(np.abs(stats.zscore(ImputedDF)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the correlation \n",
    "correlation = CorrDF.corr()\n",
    "\n",
    "relmat = correlation.iloc[:,-1]\n",
    "\n",
    "\n",
    "#relmat.columns = [\"Features\", \"Correlation with RainTomorrow\"]\n",
    "\n",
    "#print(relmat[\"columns\"])\n",
    "#rel = np.asmatrix(relmat.to_numpy())\n",
    "    \n",
    "#print(np.sort(relmat))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(16,5))\n",
    "#plt.plot(relmat)\n",
    "#plt.title(\"Spatio - Temporal Dependencies\")\n",
    "#ax.bar(relmat, color ='maroon')\n",
    "#plt.xlabel(\"Features\")\n",
    "#plt.ylabel(\"Correlation\")\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#Plotting the Correlation Heat map\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(correlation, annot= True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Plotting the Correlation between RainTommorow and other features\n",
    "abs(relmat).plot(kind='bar')\n",
    "plt.title('Correlation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of data\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(CorrDF)\n",
    "NormalizedDF = pd.DataFrame(scaler.transform(CorrDF), index = CorrDF.index, columns = CorrDF.columns)\n",
    "NormalizedDF.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING TARGET from DATASET\n",
    "target = NormalizedDF.iloc[:,-1]\n",
    "data = NormalizedDF.iloc[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OVERSAMPLING\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "data, target = oversample.fit_resample(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "target.value_counts(normalize = True).plot(kind='bar')\n",
    "plt.title('Balanced Dataset After SMOTE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and train\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.25, random_state=12345)\n",
    "\n",
    "#Calculation of Feature importance using XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "plt.barh(X_train.columns.values, xgb.feature_importances_)\n",
    "plt.xlabel(\"XGBoost Feature Importance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection Being Implemented\n",
    "features = data[['MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', \n",
    "                       'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', \n",
    "                       'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', \n",
    "                       'RainToday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test and train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "params_xgb ={'n_estimators': 500,\n",
    "            'max_depth': 16}\n",
    "xgb = XGBClassifier(**params_xgb)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "F1Score = f1_score(y_test, y_pred)\n",
    "AccuracyXGB = accuracy_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "prec, recall, fscore, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "percent=100/y_pred.shape[0]\n",
    "print(\"F1 Score: \",F1Score)\n",
    "print(\"Accuracy: \",AccuracyXGB)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(xgb, X_test, y_test)\n",
    "\n",
    "print(\"\\nTrue Positive %: \", tp*percent, \"%\\nFalse Positive %: \", fp*percent,\"%\\nFalse Negative %: \",fn*percent, \"%\\nTrue Negative %: \", tn*percent,\"%\\n\")\n",
    "print(\"\\nPrecision: \", prec, \"\\nRecall: \", recall,\"\\nF-Score: \",fscore, \"\\nSupport: \", sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning for Random Forest Classifier\n",
    "from sklearn import ensemble\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "RForest = ensemble.RandomForestClassifier()\n",
    "\n",
    "gs = GridSearchCV(RForest(), param_grid, verbose =1, cv = 2, n_jobs=-1)\n",
    "gs_results_dt = gs.fit(X_train, y_train)\n",
    "print(gs_results_dt.best_score_)\n",
    "print(gs_results_dt.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning for MLP Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "parameters = {'solver': ['lbfgs'], 'max_iter': [500,800,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000 ], 'alpha': 10.0 ** -np.arange(1, 10), 'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1,2,3,4,5,6,7,8,9]}\n",
    "gs = GridSearchCV(MLPClassifier(), parameters, verbose =1, n_jobs=-1)\n",
    "gs_results_dt = gs.fit(X_train, y_train)\n",
    "print(gs_results_dt.best_score_)\n",
    "print(gs_results_dt.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning for AdaBoost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "param_grid = {'n_estimators' : [100,200]\n",
    "             'learning_rate' : [0.001, 0.01, 0.1, 0.2, 0.5]\n",
    "             }\n",
    "\n",
    "gs = GridSearchCV(AdaBoostClassifier(), param_grid, verbose =1, n_jobs=-1)\n",
    "gs_results_dt = gs.fit(X_train, y_train)\n",
    "print(gs_results_dt.best_score_)\n",
    "print(gs_results_dt.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter tuning for GaussianNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "grid_params = {'var_smoothing':[1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3]}\n",
    "\n",
    "gs = GridSearchCV(GaussianNB(), grid_params, verbose = 1, n_jobs=-1)\n",
    "\n",
    "gs_results_nb = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs_results_nb.best_score_)\n",
    "print(gs_results_nb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning for Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "grid_params = {'penalty':['l1', 'l2'],\n",
    "              'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "gs = GridSearchCV(LogisticRegression(), grid_params, cv=5, n_jobs=-1)\n",
    "\n",
    "gs_results_lr = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs_results_lr.best_score_)\n",
    "print(gs_results_lr.best_estimator_)\n",
    "print(gs_results_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter Tuning for Decision Tree\n",
    "grid_params = {'criterion': ['gini', 'entropy'],\n",
    "'max_depth': [i for i in range(2,40,3)],\n",
    "'min_samples_leaf': [i for i in range(2, 40, 3)]}\n",
    "\n",
    "gs = GridSearchCV(DecisionTreeClassifier(), grid_params, verbose = 1, n_jobs=-1)\n",
    "\n",
    "gs_results_dt = gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs_results_dt.best_score_)\n",
    "print(gs_results_dt.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Parameter tuning for K Nearest Neighbor Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_params = {\n",
    "    'n_neighbors': [i for i in range(10,50,2)],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "\n",
    "gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=2)\n",
    "\n",
    "gs_results = gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nbg = GaussianNB()\n",
    "nbg.fit(X_train, y_train)\n",
    "y_pred = nbg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "F1Score = f1_score(y_test, y_pred)\n",
    "AccuracyNB = accuracy_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "prec, recall, fscore, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "percent=100/y_pred.shape[0]\n",
    "print(\"F1 Score: \",F1Score)\n",
    "print(\"Accuracy: \",AccuracyNB)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(nbg, X_test, y_test)\n",
    "\n",
    "print(\"\\nTrue Positive %: \", tp*percent, \"%\\nFalse Positive %: \", fp*percent,\"%\\nFalse Negative %: \",fn*percent, \"%\\nTrue Negative %: \", tn*percent,\"%\\n\")\n",
    "print(\"\\nPrecision: \", prec, \"\\nRecall: \", recall,\"\\nF-Score: \",fscore, \"\\nSupport: \", sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "y_pred = dtc.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "F1Score = f1_score(y_test, y_pred)\n",
    "AccuracyDT = accuracy_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "prec, recall, fscore, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "percent=100/y_pred.shape[0]\n",
    "print(\"F1 Score: \",F1Score)\n",
    "print(\"Accuracy: \",AccuracyDT)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(dtc, X_test, y_test)\n",
    "\n",
    "print(\"\\nTrue Positive %: \", tp*percent, \"%\\nFalse Positive %: \", fp*percent,\"%\\nFalse Negative %: \",fn*percent, \"%\\nTrue Negative %: \", tn*percent,\"%\\n\")\n",
    "print(\"\\nPrecision: \", prec, \"\\nRecall: \", recall,\"\\nF-Score: \",fscore, \"\\nSupport: \", sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X_train, y_train)\n",
    "y_pred = logr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "F1Score = f1_score(y_test, y_pred)\n",
    "AccuracyLR = accuracy_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "prec, recall, fscore, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "percent=100/y_pred.shape[0]\n",
    "print(\"F1 Score: \",F1Score)\n",
    "print(\"Accuracy: \",AccuracyLR)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(logr, X_test, y_test)\n",
    "\n",
    "print(\"\\nTrue Positive %: \", tp*percent, \"%\\nFalse Positive %: \", fp*percent,\"%\\nFalse Negative %: \",fn*percent, \"%\\nTrue Negative %: \", tn*percent,\"%\\n\")\n",
    "print(\"\\nPrecision: \", prec, \"\\nRecall: \", recall,\"\\nF-Score: \",fscore, \"\\nSupport: \", sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "F1Score = f1_score(y_test, y_pred)\n",
    "AccuracyKNN = accuracy_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "prec, recall, fscore, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "percent=100/y_pred.shape[0]\n",
    "print(\"F1 Score: \",F1Score)\n",
    "print(\"Accuracy: \",AccuracyKNN)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(knn, X_test, y_test)\n",
    "\n",
    "print(\"\\nTrue Positive %: \", tp*percent, \"%\\nFalse Positive %: \", fp*percent,\"%\\nFalse Negative %: \",fn*percent, \"%\\nTrue Negative %: \", tn*percent,\"%\\n\")\n",
    "print(\"\\nPrecision: \", prec, \"\\nRecall: \", recall,\"\\nF-Score: \",fscore, \"\\nSupport: \", sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP Classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "params_nn = {'hidden_layer_sizes': (30,30,30),\n",
    "             'activation': 'logistic',\n",
    "             'solver': 'lbfgs',\n",
    "             'max_iter': 500}\n",
    "\n",
    "multi_layer_perceptron = MLPClassifier()\n",
    "multi_layer_perceptron.fit(X_train, y_train)\n",
    "y_pred = multi_layer_perceptron.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "F1Score = f1_score(y_test, y_pred)\n",
    "AccuracyMLP = accuracy_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "prec, recall, fscore, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "percent=100/y_pred.shape[0]\n",
    "print(\"F1 Score: \",F1Score)\n",
    "print(\"Accuracy: \",AccuracyMLP)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(multi_layer_perceptron, X_test, y_test)\n",
    "\n",
    "print(\"\\nTrue Positive %: \", tp*percent, \"%\\nFalse Positive %: \", fp*percent,\"%\\nFalse Negative %: \",fn*percent, \"%\\nTrue Negative %: \", tn*percent,\"%\\n\")\n",
    "print(\"\\nPrecision: \", prec, \"\\nRecall: \", recall,\"\\nF-Score: \",fscore, \"\\nSupport: \", sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "F1Score = f1_score(y_test, y_pred)\n",
    "AccuracyAB = accuracy_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "prec, recall, fscore, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "percent=100/y_pred.shape[0]\n",
    "print(\"F1 Score: \",F1Score)\n",
    "print(\"Accuracy: \",AccuracyAB)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(ada, X_test, y_test)\n",
    "\n",
    "print(\"\\nTrue Positive %: \", tp*percent, \"%\\nFalse Positive %: \", fp*percent,\"%\\nFalse Negative %: \",fn*percent, \"%\\nTrue Negative %: \", tn*percent,\"%\\n\")\n",
    "print(\"\\nPrecision: \", prec, \"\\nRecall: \", recall,\"\\nF-Score: \",fscore, \"\\nSupport: \", sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn import ensemble\n",
    "\n",
    "params_rf = {'max_depth': 16,\n",
    "             'min_samples_leaf': 1,\n",
    "             'min_samples_split': 2,\n",
    "             'n_estimators': 100,\n",
    "             'random_state': 12345}\n",
    "\n",
    "RForest = ensemble.RandomForestClassifier()\n",
    "\n",
    "#Fitting the train dataset\n",
    "train= RForest.fit(X_train, y_train)\n",
    "         \n",
    "#Predict the test dataset\n",
    "y_pred = RForest.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support\n",
    "F1Score = f1_score(y_test, y_pred)\n",
    "AccuracyRF = accuracy_score(y_test, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "prec, recall, fscore, sup = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "percent=100/y_pred.shape[0]\n",
    "print(\"F1 Score: \",F1Score)\n",
    "print(\"Accuracy: \",AccuracyRF)\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "plot_confusion_matrix(RForest, X_test, y_test)\n",
    "\n",
    "print(\"\\nTrue Positive %: \", tp*percent, \"%\\nFalse Positive %: \", fp*percent,\"%\\nFalse Negative %: \",fn*percent, \"%\\nTrue Negative %: \", tn*percent,\"%\\n\")\n",
    "print(\"\\nPrecision: \", prec, \"\\nRecall: \", recall,\"\\nF-Score: \",fscore, \"\\nSupport: \", sup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Graph for model comparision\n",
    "accuracy_scores = [AccuracyLR, AccuracyKNN, AccuracyNB, AccuracyDT, AccuracyRF, AccuracyMLP, AccuracyAB, AccuracyXGB]\n",
    "model_name = ['Logistic Regression','KNN','Naive Bayes','Decision Tree','Random Forest','MLPerceptron','AdaBoost','XGBoost']\n",
    "\n",
    "plt.subplots(figsize=(12,10))\n",
    "sns.barplot(model_name, accuracy_scores, palette='Paired')\n",
    "plt.title(\"Model Comparision\",fontsize=13)\n",
    "plt.xlabel(\"Model Name\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
